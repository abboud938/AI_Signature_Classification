{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kagglehub\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16 , ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D , Input , Dense , Dropout , LayerNormalization , Add\n",
    "from tensorflow.keras import Model , layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping , ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"akashgundu/signature-verification-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for dir in os.listdir(os.path.join(path , 'extract')):\n",
    "  for sub_dir in os.listdir(os.path.join(path , 'extract' , dir)):\n",
    "    if sub_dir.endswith('.jpg'):\n",
    "      data.append({'image_path': os.path.join(path , 'extract' , dir , sub_dir), 'person_id': dir})\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "grouped = data.groupby('person_id').agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, label=''):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image not found at {image_path}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = image / 255.0\n",
    "    return np.array(image, dtype=np.float32)\n",
    "\n",
    "def generate_triplets(number_of_triplets):\n",
    "    anchor = []\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for _ in range(number_of_triplets):\n",
    "        index = random.sample(range(10), 3)\n",
    "        genuine_rows = grouped[grouped[\"person_id\"].apply(lambda x: len(x.split(\"_\")) == 1)]\n",
    "        random_genuine_row = genuine_rows.sample(1)\n",
    "\n",
    "        first_genuine_image = load_image(random_genuine_row['image_path'].iloc[0][index[0]])\n",
    "        second_genuine_image = load_image(random_genuine_row['image_path'].iloc[0][index[1]])\n",
    "        forged_rows = grouped[grouped[\"person_id\"].apply(lambda x: len(x.split(\"_\")) != 1)]\n",
    "        if len(forged_rows) < 1:\n",
    "            raise ValueError(\"Not enough forged rows to sample from.\")\n",
    "        random_forged_row = forged_rows.sample(1)\n",
    "\n",
    "        forged_image = load_image(random_forged_row['image_path'].iloc[0][index[2]])\n",
    "        anchor.append(first_genuine_image)\n",
    "        positive.append(second_genuine_image)\n",
    "        negative.append(forged_image)\n",
    "\n",
    "    return np.array(anchor), np.array(positive), np.array(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha=0.5):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    # Calculate Euclidean distance between anchor and positive, and anchor and negative\n",
    "    distance1 = tf.sqrt(tf.reduce_sum(tf.square(anchor - positive), axis=-1))\n",
    "    distance2 = tf.sqrt(tf.reduce_sum(tf.square(anchor - negative), axis=-1))\n",
    "\n",
    "    # Compute the triplet loss with margin alpha\n",
    "    loss = tf.reduce_mean(tf.maximum(distance1 - distance2 + alpha, 0))\n",
    "    print(distance1)\n",
    "    print(distance2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_embedding_model(embedding_dim=128, input_shape=(224, 224, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # CNN layers\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D(2, 2)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2, 2)(x)\n",
    "\n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2, 2)(x)\n",
    "\n",
    "    x = layers.Conv2D(512, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.MaxPooling2D(2, 2)(x)\n",
    "\n",
    "    x = layers.Conv2D(1024, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.MaxPooling2D(2, 2)(x)\n",
    "\n",
    "    embeddings = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=embeddings)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_cnn_embedding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "batch_size = 100\n",
    "n_of_samples = 1000\n",
    "a , p , n = generate_triplets(n_of_samples) #return np.array(anchors) , np.array(positives) , np.array(negatives)\n",
    "model.compile(loss=triplet_loss,optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "y_train = np.zeros((n_of_samples, 3))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((a, p, n)).batch(batch_size).shuffle(buffer_size=1024)\n",
    "history = model.fit(dataset,epochs=30,callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loading the CNN To get the embeddings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "genuine_embedding = []\n",
    "forged_embedding = []\n",
    "genuine_rows = grouped[grouped[\"person_id\"].apply(lambda x: len(x.split(\"_\")) == 1)]\n",
    "forged_rows = grouped[grouped[\"person_id\"].apply(lambda x: len(x.split(\"_\")) != 1)]\n",
    "genuine_labels = np.ones(len(genuine_rows))\n",
    "forged_labels = np.zeros(len(forged_rows))\n",
    "for index , row in genuine_rows.iterrows():\n",
    "  image = load_image(row['image_path'][0])\n",
    "  embedding = model.predict(np.expand_dims(image, axis=0))\n",
    "  genuine_embedding.append(embedding[0])\n",
    "for index , row in forged_rows.iterrows():\n",
    "  image = load_image(row['image_path'][0])\n",
    "  embedding = model.predict(np.expand_dims(image, axis=0))\n",
    "  forged_embedding.append(embedding[0])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genuine_labels = np.array(genuine_labels).reshape(-1, 1)\n",
    "forged_labels = np.array(forged_labels).reshape(-1, 1)\n",
    "genuine_signatures = np.hstack((np.squeeze(np.array(genuine_embedding)) , np.array(genuine_labels)))\n",
    "forged_signatures = np.hstack((np.squeeze(np.array(forged_embedding)), np.array(forged_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both datasets\n",
    "final_dataset = np.vstack((genuine_signatures, forged_signatures))\n",
    "# Shuffle the dataset\n",
    "np.random.shuffle(final_dataset)\n",
    "# Separate features (X) and labels (y)\n",
    "X = final_dataset[:, :-1]  # Features\n",
    "y = final_dataset[:, -1]   # Labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "df_train = pd.DataFrame(X_train)\n",
    "df_train['label'] = y_train\n",
    "df_train.to_csv('/content/drive/MyDrive/train_data.csv', index=False)\n",
    "\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_test['label'] = y_test\n",
    "df_test.to_csv('/content/drive/MyDrive/test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
